# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y8O1q3d7PCHcq7xQwddmDf6ZOJkl3YOG

# **Assignment 1:MLP for Image Classification:Connectionist AI**

Imports and Data loading
"""

import numpy as np
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout

(x_train, y_train), (x_test, y_test) = mnist.load_data()

"""Extracting labels"""

total_labels = len(np.unique(y_train))
print(f"labels:{np.unique(y_train)}")

"""One-Hot encoding for classification"""

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

"""Data preprocessing"""

input_size = x_train.shape[1]**2
x_train = np.reshape(x_train, (-1,input_size))
x_train = x_train.astype('float32') / 255

x_test = np.reshape(x_test, (-1,input_size))
x_test = x_test.astype('float32') / 255

print(f"x_train:{x_train.shape}")
print(f"x_test:{x_test.shape}")

"""Hyperparams"""

batch_size = 64
hidden_units = 256
dropout = 0.35

"""The Traditional MLP model"""

model = Sequential()

model.add(Dense(hidden_units, input_dim=input_size))
model.add(Activation('relu'))
model.add(Dropout(dropout))

model.add(Dense(hidden_units))
model.add(Activation('relu'))
model.add(Dropout(dropout))

model.add(Dense(total_labels))
model.add(Activation('softmax'))
model.summary()

"""MLP Model Training"""

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10, batch_size=batch_size)

"""MLP model evaluation"""

_, acc = model.evaluate(x_test,
                        y_test,
                        batch_size=batch_size,
                        verbose=0)
print(f"Accuracy: {100.0 * acc}%")

"""---
## Traditional MLP Accuracy: 98.2%
---

## MLP-Mixer model

Arguments(HyperParameters)
"""

patch_size = 4
num_patches = (28 // patch_size) ** 2
# print( (input_size // patch_size) ** 2)
hidden_units=64
learning_speed=0.05
# print(num_patches)

"""Imports"""

from tensorflow.keras.layers import LayerNormalization, Layer, Input, GlobalAveragePooling1D, Dropout
from tensorflow.keras import Model
import tensorflow.keras as keras
from tensorflow import transpose,reshape,shape
from tensorflow.image import extract_patches

"""The Mixer Layer"""

class MixerLayer(Layer):
    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.mlp1 = Sequential(
            [
                Dense(units=num_patches, activation="gelu"),
                Dense(units=num_patches),
                Dropout(rate=dropout),
            ]
        )
        self.mlp2 = Sequential(
            [
                Dense(units=num_patches, activation="gelu"),
                Dense(units=hidden_units),
                Dropout(rate=dropout),
            ]
        )
        self.normalize = LayerNormalization(epsilon=1e-6)

    def build(self, input_shape):
        return super().build(input_shape)

    def call(self, inputs):
        x = self.normalize(inputs)
        x_channels = transpose(x, perm=(0, 2, 1))
        mlp1_outputs = self.mlp1(x_channels)
        mlp1_outputs = transpose(mlp1_outputs, perm=(0, 2, 1))
        x = mlp1_outputs + inputs
        x_patches = self.normalize(x)
        mlp2_outputs = self.mlp2(x_patches)
        x = x + mlp2_outputs
        return x

"""The Patch extraction Layer"""

class PatchesLayer(Layer):
    def __init__(self, patch_size, **kwargs):
        super().__init__(**kwargs)
        self.patch_size = patch_size

    def call(self, x):
        patches = extract_patches(images=x, sizes=[1, self.patch_size, self.patch_size, 1], strides=[1, self.patch_size, self.patch_size, 1], rates=[1, 1, 1, 1], padding='VALID')
        batch_size = shape(patches)[0]
        num_patches = shape(patches)[1] * shape(patches)[2]
        patch_dim =shape(patches)[3]
        output = reshape(patches, (batch_size, num_patches, patch_dim))
        return output

"""Model Maker"""

def model_maker(blocks):
    #The input layer
    inputs = Input(shape=(28,28,1))
    #The patch extraction layer
    patches = PatchesLayer(patch_size)(inputs)
    x = Dense(units=hidden_units)(patches)
    x = blocks(x)
    #The average pooling layer
    condensed_input = GlobalAveragePooling1D()(x)
    condensed_input = Dropout(rate=dropout)(condensed_input)
    #The Output layer
    logits = Dense(total_labels,activation='softmax')(condensed_input)
    return Model(inputs=inputs, outputs=logits)

"""Building of the model using the Patch and"""

mixer_blocks = Sequential(
    [MixerLayer(num_patches, hidden_units, dropout) for _ in range(4)]
)
learning_rate = 0.005
mixer_model = model_maker(mixer_blocks)

"""Loading of dataset for the Mixer Model"""

(x_train_res, y_train_res), (x_test_res, y_test_res) = mnist.load_data()
x_train_res = x_train_res.reshape((-1, 28, 28, 1))
x_test_res = x_test_res.reshape((-1, 28, 28, 1))
x_train_res = x_train_res.astype('float32') / 255
x_test_res = x_test_res.astype('float32') / 255

mixer_model.summary()

"""Mixer Model Training"""

mixer_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
mixer_model.fit(x_train_res, y_train_res, epochs=10, batch_size=64)

"""Mixer Model Evaluation"""

_, acc_mixer = mixer_model.evaluate(x_test_res, y_test_res, batch_size=64, verbose=0)
print(f"Mixer MLP Accuracy: {100*acc}%")

"""---
## Mixer-MLP accuracy: 97.4%
---

Looking at the models from various perspectives it is clear that the Traditional MLP outperforms the MLP mixer model by a difference of 0.9%.
The reasons being the overall complexity of the task and the complexity of the models: where Traditional MLP is a much simpler model and would perform good on simple tasks the Mixer model is not designed for such low level tasks. And would thus beat the Traditional model on other tasks. Also it can be pretrained to further diversify it's usage and improve overall accuracy.

##References
- https://www.analyticsvidhya.com/blog/2020/12/mlp-multilayer-perceptron-simple-overview/
- https://www.v7labs.com/blog/neural-networks-activation-functions
- https://www.baeldung.com/cs/learning-rate-batch-size#:~:text=Batch%20size%20defines%20the%20number,training%20set%20in%20one%20epoch.
- https://arxiv.org/abs/2105.03404v2
- https://sh-tsang.medium.com/review-resmlp-feedforward-networks-for-image-classification-with-data-efficient-training-4eeb1eb5efa6
- https://stats.stackexchange.com/questions/162988/why-sigmoid-function-instead-of-anything-else/318209#318209
"""